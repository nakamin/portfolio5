{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XceptionNet„ÅÆÂÆüË£Ö"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available: True\n",
      "GPU Count: 1\n",
      "GPU Name: NVIDIA GeForce RTX 4070 Ti SUPER\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(\"CUDA Available:\", torch.cuda.is_available())\n",
    "print(\"GPU Count:\", torch.cuda.device_count())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU Name:\", torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import accimage\n",
    "import os\n",
    "from PIL import Image\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "# ÂÜçÁèæÊÄß„ÅÆ„Åü„ÇÅ„Å´„É©„É≥„ÉÄ„É†„Ç∑„Éº„Éâ„ÇíË®≠ÂÆö\n",
    "def set_seed(seed):\n",
    "    random.seed(seed) # Python\n",
    "    np.random.seed(seed) # NumPy\n",
    "    torch.manual_seed(seed) # PyTorch\n",
    "    \n",
    "    # GPU„Çí‰Ωø„ÅÜÂ†¥Âêà\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)  # ‰ªªÊÑè„ÅÆ„Ç∑„Éº„ÉâÂÄ§"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- acciimage„ÅØtorchvision„ÅÆÁîªÂÉèË™≠„ÅøËæº„Åø„Éê„ÉÉ„ÇØ„Ç®„É≥„Éâ\n",
    "- Áõ¥Êé•Image()„Çí‰Ωø„Å£„Å¶ÁîªÂÉè„ÇíË™≠„ÅøËæº„ÇÄ„ÅÆ„Åß„ÅØ„Å™„Åè„ÄÅPIL‰∫íÊèõ„ÅÆ„Ç™„Éñ„Ç∏„Çß„ÇØ„Éà„Å®„Åó„Å¶Êâ±„ÅÜÂøÖË¶Å„Åå„ÅÇ„Çã"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mÁèæÂú®„ÅÆ„Çª„É´„Åæ„Åü„ÅØÂâç„ÅÆ„Çª„É´„Åß„Ç≥„Éº„Éâ„ÇíÂÆüË°å‰∏≠„Å´„ÄÅ„Ç´„Éº„Éç„É´ (Kernel) „Åå„ÇØ„É©„ÉÉ„Ç∑„É•„Åó„Åæ„Åó„Åü„ÄÇ\n",
      "\u001b[1;31m„Ç®„É©„Éº„ÅÆÂéüÂõ†„ÇíÁâπÂÆö„Åô„Çã„Å´„ÅØ„ÄÅ„Çª„É´ÂÜÖ„ÅÆ„Ç≥„Éº„Éâ„ÇíÁ¢∫Ë™ç„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ\n",
      "\u001b[1;31mË©≥Á¥∞„Å´„Å§„ÅÑ„Å¶„ÅØ<a href='https://aka.ms/vscodeJupyterKernelCrash'>„Åì„Å°„Çâ</a>„Çí„ÇØ„É™„ÉÉ„ÇØ„Åó„Åæ„Åô„ÄÇ\n",
      "\u001b[1;31mË©≥Á¥∞„Å´„Å§„ÅÑ„Å¶„ÅØ„ÄÅJupyter <a href='command:jupyter.viewOutput'>„É≠„Ç∞</a> „ÇíÂèÇÁÖß„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "image_path = \"./data/images\"\n",
    "\n",
    "# ÁîªÂÉè„ÅÆÂâçÂá¶ÁêÜ\n",
    "transform = transforms.Compose([\n",
    "    #transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "image_files = [f for f in os.listdir(image_path) if f.endswith(('.jpg'))]\n",
    "\n",
    "image_tensors = []\n",
    "\n",
    "for file in image_files:\n",
    "    img = accimage.Image(os.path.join(image_path, file)) # PIL„ÅßÁîªÂÉè„ÇíÈñã„Åè\n",
    "    img = transform(img) # ÂâçÂá¶ÁêÜ„ÇíÈÅ©Áî®\n",
    "    image_tensors.append(img)\n",
    "\n",
    "image_tensors = torch.stack(image_tensors)\n",
    "\n",
    "print(f\"Loaded {len(image_tensors)} images as tensors.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_tensors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'image_tensors' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[74], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# PyTorch„ÅÆ CHW„Éï„Ç©„Éº„Éû„ÉÉ„Éà„Å´„Å™„Å£„Å¶„ÅÑ„Çã„ÅÆ„Åß„ÄÅNumPyÂΩ¢Âºè„ÅÆ NHWC„Éï„Ç©„Éº„Éû„ÉÉ„Éà„Å´Â§âÊèõ„Åô„Çã\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m image_tensors \u001b[38;5;241m=\u001b[39m image_tensors\u001b[38;5;241m.\u001b[39mnumpy()  \u001b[38;5;66;03m# Tensor ‚Üí NumPy\u001b[39;00m\n\u001b[1;32m      3\u001b[0m image_tensors_numpy \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mtranspose(image_tensors, (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m))  \u001b[38;5;66;03m# CHW ‚Üí HWC (Num_samples, Height, Width, Channels)\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(image_tensors_numpy\u001b[38;5;241m.\u001b[39mshape)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'image_tensors' is not defined"
     ]
    }
   ],
   "source": [
    "# PyTorch„ÅÆ CHW„Éï„Ç©„Éº„Éû„ÉÉ„Éà„Å´„Å™„Å£„Å¶„ÅÑ„Çã„ÅÆ„Åß„ÄÅNumPyÂΩ¢Âºè„ÅÆ NHWC„Éï„Ç©„Éº„Éû„ÉÉ„Éà„Å´Â§âÊèõ„Åô„Çã\n",
    "image_tensors = image_tensors.numpy()  # Tensor ‚Üí NumPy\n",
    "image_tensors_numpy = np.transpose(image_tensors, (0, 2, 3, 1))  # CHW ‚Üí HWC (Num_samples, Height, Width, Channels)\n",
    "print(image_tensors_numpy.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lesion_id</th>\n",
       "      <th>image_id</th>\n",
       "      <th>dx</th>\n",
       "      <th>dx_type</th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>localization</th>\n",
       "      <th>dataset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HAM_0000118</td>\n",
       "      <td>ISIC_0027419</td>\n",
       "      <td>bkl</td>\n",
       "      <td>histo</td>\n",
       "      <td>80.0</td>\n",
       "      <td>male</td>\n",
       "      <td>scalp</td>\n",
       "      <td>vidir_modern</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HAM_0000118</td>\n",
       "      <td>ISIC_0025030</td>\n",
       "      <td>bkl</td>\n",
       "      <td>histo</td>\n",
       "      <td>80.0</td>\n",
       "      <td>male</td>\n",
       "      <td>scalp</td>\n",
       "      <td>vidir_modern</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HAM_0002730</td>\n",
       "      <td>ISIC_0026769</td>\n",
       "      <td>bkl</td>\n",
       "      <td>histo</td>\n",
       "      <td>80.0</td>\n",
       "      <td>male</td>\n",
       "      <td>scalp</td>\n",
       "      <td>vidir_modern</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HAM_0002730</td>\n",
       "      <td>ISIC_0025661</td>\n",
       "      <td>bkl</td>\n",
       "      <td>histo</td>\n",
       "      <td>80.0</td>\n",
       "      <td>male</td>\n",
       "      <td>scalp</td>\n",
       "      <td>vidir_modern</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HAM_0001466</td>\n",
       "      <td>ISIC_0031633</td>\n",
       "      <td>bkl</td>\n",
       "      <td>histo</td>\n",
       "      <td>75.0</td>\n",
       "      <td>male</td>\n",
       "      <td>ear</td>\n",
       "      <td>vidir_modern</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10010</th>\n",
       "      <td>HAM_0002867</td>\n",
       "      <td>ISIC_0033084</td>\n",
       "      <td>akiec</td>\n",
       "      <td>histo</td>\n",
       "      <td>40.0</td>\n",
       "      <td>male</td>\n",
       "      <td>abdomen</td>\n",
       "      <td>vidir_modern</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10011</th>\n",
       "      <td>HAM_0002867</td>\n",
       "      <td>ISIC_0033550</td>\n",
       "      <td>akiec</td>\n",
       "      <td>histo</td>\n",
       "      <td>40.0</td>\n",
       "      <td>male</td>\n",
       "      <td>abdomen</td>\n",
       "      <td>vidir_modern</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10012</th>\n",
       "      <td>HAM_0002867</td>\n",
       "      <td>ISIC_0033536</td>\n",
       "      <td>akiec</td>\n",
       "      <td>histo</td>\n",
       "      <td>40.0</td>\n",
       "      <td>male</td>\n",
       "      <td>abdomen</td>\n",
       "      <td>vidir_modern</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10013</th>\n",
       "      <td>HAM_0000239</td>\n",
       "      <td>ISIC_0032854</td>\n",
       "      <td>akiec</td>\n",
       "      <td>histo</td>\n",
       "      <td>80.0</td>\n",
       "      <td>male</td>\n",
       "      <td>face</td>\n",
       "      <td>vidir_modern</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10014</th>\n",
       "      <td>HAM_0003521</td>\n",
       "      <td>ISIC_0032258</td>\n",
       "      <td>mel</td>\n",
       "      <td>histo</td>\n",
       "      <td>70.0</td>\n",
       "      <td>female</td>\n",
       "      <td>back</td>\n",
       "      <td>vidir_modern</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10015 rows √ó 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         lesion_id      image_id     dx dx_type   age     sex localization  \\\n",
       "0      HAM_0000118  ISIC_0027419    bkl   histo  80.0    male        scalp   \n",
       "1      HAM_0000118  ISIC_0025030    bkl   histo  80.0    male        scalp   \n",
       "2      HAM_0002730  ISIC_0026769    bkl   histo  80.0    male        scalp   \n",
       "3      HAM_0002730  ISIC_0025661    bkl   histo  80.0    male        scalp   \n",
       "4      HAM_0001466  ISIC_0031633    bkl   histo  75.0    male          ear   \n",
       "...            ...           ...    ...     ...   ...     ...          ...   \n",
       "10010  HAM_0002867  ISIC_0033084  akiec   histo  40.0    male      abdomen   \n",
       "10011  HAM_0002867  ISIC_0033550  akiec   histo  40.0    male      abdomen   \n",
       "10012  HAM_0002867  ISIC_0033536  akiec   histo  40.0    male      abdomen   \n",
       "10013  HAM_0000239  ISIC_0032854  akiec   histo  80.0    male         face   \n",
       "10014  HAM_0003521  ISIC_0032258    mel   histo  70.0  female         back   \n",
       "\n",
       "            dataset  \n",
       "0      vidir_modern  \n",
       "1      vidir_modern  \n",
       "2      vidir_modern  \n",
       "3      vidir_modern  \n",
       "4      vidir_modern  \n",
       "...             ...  \n",
       "10010  vidir_modern  \n",
       "10011  vidir_modern  \n",
       "10012  vidir_modern  \n",
       "10013  vidir_modern  \n",
       "10014  vidir_modern  \n",
       "\n",
       "[10015 rows x 8 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata = pd.read_csv(\"./data/HAM10000_metadata\")\n",
    "metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10015 entries, 0 to 10014\n",
      "Data columns (total 8 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   lesion_id     10015 non-null  object \n",
      " 1   image_id      10015 non-null  object \n",
      " 2   dx            10015 non-null  object \n",
      " 3   dx_type       10015 non-null  object \n",
      " 4   age           9958 non-null   float64\n",
      " 5   sex           10015 non-null  object \n",
      " 6   localization  10015 non-null  object \n",
      " 7   dataset       10015 non-null  object \n",
      "dtypes: float64(1), object(7)\n",
      "memory usage: 626.1+ KB\n"
     ]
    }
   ],
   "source": [
    "metadata.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>dx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ISIC_0027419</td>\n",
       "      <td>bkl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ISIC_0025030</td>\n",
       "      <td>bkl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ISIC_0026769</td>\n",
       "      <td>bkl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ISIC_0025661</td>\n",
       "      <td>bkl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ISIC_0031633</td>\n",
       "      <td>bkl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10010</th>\n",
       "      <td>ISIC_0033084</td>\n",
       "      <td>akiec</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10011</th>\n",
       "      <td>ISIC_0033550</td>\n",
       "      <td>akiec</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10012</th>\n",
       "      <td>ISIC_0033536</td>\n",
       "      <td>akiec</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10013</th>\n",
       "      <td>ISIC_0032854</td>\n",
       "      <td>akiec</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10014</th>\n",
       "      <td>ISIC_0032258</td>\n",
       "      <td>mel</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10015 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           image_id     dx\n",
       "0      ISIC_0027419    bkl\n",
       "1      ISIC_0025030    bkl\n",
       "2      ISIC_0026769    bkl\n",
       "3      ISIC_0025661    bkl\n",
       "4      ISIC_0031633    bkl\n",
       "...             ...    ...\n",
       "10010  ISIC_0033084  akiec\n",
       "10011  ISIC_0033550  akiec\n",
       "10012  ISIC_0033536  akiec\n",
       "10013  ISIC_0032854  akiec\n",
       "10014  ISIC_0032258    mel\n",
       "\n",
       "[10015 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata_labels = metadata[['image_id', 'dx']]\n",
    "metadata_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        2\n",
       "1        2\n",
       "2        2\n",
       "3        2\n",
       "4        2\n",
       "        ..\n",
       "10010    0\n",
       "10011    0\n",
       "10012    0\n",
       "10013    0\n",
       "10014    4\n",
       "Name: dx, Length: 10015, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = metadata_labels['dx'].map({\"akiec\":0, \"bcc\":1, \"bkl\":2, \"df\":3, \"mel\":4, \"nv\":5, \"vasc\":6})\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10010</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10011</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10012</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10013</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10014</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10015 rows √ó 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0  1  2  3  4  5  6\n",
       "0      0  0  1  0  0  0  0\n",
       "1      0  0  1  0  0  0  0\n",
       "2      0  0  1  0  0  0  0\n",
       "3      0  0  1  0  0  0  0\n",
       "4      0  0  1  0  0  0  0\n",
       "...   .. .. .. .. .. .. ..\n",
       "10010  1  0  0  0  0  0  0\n",
       "10011  1  0  0  0  0  0  0\n",
       "10012  1  0  0  0  0  0  0\n",
       "10013  1  0  0  0  0  0  0\n",
       "10014  0  0  0  0  1  0  0\n",
       "\n",
       "[10015 rows x 7 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_encoding = pd.get_dummies(labels, dtype=int)\n",
    "labels_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‰ΩøÁî®‰∏≠„É°„É¢„É™: 13.97 GB\n"
     ]
    }
   ],
   "source": [
    "import psutil\n",
    "print(f\"‰ΩøÁî®‰∏≠„É°„É¢„É™: {psutil.virtual_memory().used / (1024 ** 3):.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# „É°„É¢„É™„ÅÆÂâäÊ∏õ\n",
    "del image_tensors\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10015, 224, 224, 3)\n",
      "5.616016387939453 GB\n"
     ]
    }
   ],
   "source": [
    "print(image_tensors_numpy.shape)\n",
    "print(image_tensors_numpy.nbytes / (1024 ** 3), \"GB\")  # „É°„É¢„É™‰ΩøÁî®ÈáèÔºàGBÂçò‰ΩçÔºâ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_data, val_data, train_labels, val_labels = train_test_split(\n",
    "    image_tensors_numpy, labels, test_size=0.3, stratify=labels, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data shape: (7010, 224, 224, 3)\n",
      "train_labels shape: (7010,)\n",
      "val_data shape: (3005, 224, 224, 3)\n",
      "val_labels shape: (3005,)\n"
     ]
    }
   ],
   "source": [
    "print(\"train_data shape:\", train_data.shape)\n",
    "print(\"train_labels shape:\", train_labels.shape)\n",
    "print(\"val_data shape:\", val_data.shape)\n",
    "print(\"val_labels shape:\", val_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names_mapping = {\n",
    "    0: \"AKIEC\",\n",
    "    1: \"BCC\",\n",
    "    2: \"BKL\",\n",
    "    3: \"DF\",\n",
    "    4: \"MEL\",\n",
    "    5: \"NV\",\n",
    "    6: \"VASC\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     327\n",
       "1     514\n",
       "2    1099\n",
       "3     115\n",
       "4    1113\n",
       "5    6705\n",
       "6     142\n",
       "dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_counts = np.sum(labels_encoding, axis=0)\n",
    "class_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XceptionNet „ÅÆ‰∫ãÂâçÂ≠¶ÁøíÊ∏à„Åø„É¢„Éá„É´„ÇíÈÅ©Áî®"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Â≠¶Áøí„Éá„Éº„Çø„ÅÆÊã°Âºµ„ÅØImageNet„Å®„Åù„Çç„Åà„Çã"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- „Åì„ÅÆ„ÅÇ„Å®„ÄÅÊôÆÈÄö„Å´ÈÅ©Áî®„Å®„Åó„Åü„Çâ„Ç®„É©„Éº„Å´„Å™„Çã„ÅÆ„Åß„ÄÅDataset„ÇØ„É©„Çπ„Çí‰ΩúÊàê„Åó„Å¶ÈÅ©Áî®„Åï„Åõ„Çã\n",
    "- torchvision.transforms „ÅÆ ToTensor() „ÅØ PILÁîªÂÉè „Åæ„Åü„ÅØ numpy.ndarray (H, W, C) „ÅÆ„Åø„ÇíÂá¶ÁêÜ„Åß„Åç„Çã"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, labels, transform):\n",
    "        \"\"\"\n",
    "        data„ÅØnumpy.darrray„ÅÆÁîªÂÉè„Éá„Éº„ÇøÔºàN, H, W, C)\n",
    "        labels„ÅØnumpy.darray„ÅÆ„É©„Éô„É´„Éá„Éº„ÇøÔºàN, Ôºâ\n",
    "        transform„ÅØÂÆöÁæ©„Åó„ÅüÁîªÂÉèÂ§âÊèõÂá¶ÁêÜ\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = self.data[idx]  # numpy.ndarray (H, W, C)\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # NumPy ‚Üí PIL „Å´Â§âÊèõÔºàToTensor() „ÅÆ„Åü„ÇÅÔºâ\n",
    "        image = Image.fromarray((image * 255).astype(\"uint8\"))\n",
    "\n",
    "        # transform „ÇíÈÅ©Áî®\n",
    "        if self.transform:\n",
    "            image = self.transform(image)  # `ToTensor()` „Å´„Çà„Çä (C, H, W) „Å´Â§âÊèõ\n",
    "\n",
    "        return image, torch.tensor(label, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'values'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[59], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_labels \u001b[38;5;241m=\u001b[39m train_labels\u001b[38;5;241m.\u001b[39mvalues  \u001b[38;5;66;03m# Pandas Series ‚Üí NumPy ÈÖçÂàó\u001b[39;00m\n\u001b[1;32m      2\u001b[0m val_labels \u001b[38;5;241m=\u001b[39m val_labels\u001b[38;5;241m.\u001b[39mvalues\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'values'"
     ]
    }
   ],
   "source": [
    "train_labels = train_labels.values  # Pandas Series ‚Üí NumPy ÈÖçÂàó\n",
    "val_labels = val_labels.values      # Ê§úË®º„Éá„Éº„Çø„ÇÇÂêåÊßò„Å´Â§âÊèõ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- torch.longÂûã„ÅØÊï¥Êï∞Á≥ªÔºàint64Ôºâ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_dataset = CustomDataset(train_data, train_labels, transform=train_transform)\n",
    "val_dataset = CustomDataset(val_data, val_labels, transform=val_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CPU„ÅÆ„Ç≥„Ç¢Êï∞„ÇíÁ¢∫Ë™ç\n",
    "import os\n",
    "os.cpu_count()  # „Ç≥„Ç¢Êï∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ë®ìÁ∑¥Áî®„Éá„Éº„Çø\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=2, pin_memory=True)\n",
    "# Ê§úË®ºÁî®„Éá„Éº„Çø\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, num_workers=2, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x7f09ed077950>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape: torch.Size([16, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "for imgs, labels in train_loader:\n",
    "    print(\"Image shape:\", imgs.shape)  # ÊúüÂæÖ„Åô„ÇãÂΩ¢: (batch_size, C, H, W)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='/works/mlruns/1', creation_time=1741694676407, experiment_id='1', last_update_time=1741694676407, lifecycle_stage='active', name='250311_xceptionnet', tags={}>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "\n",
    "# MLflow „ÅÆ„Éà„É©„ÉÉ„Ç≠„É≥„Ç∞ URI „ÇíË®≠ÂÆö\n",
    "mlflow.set_tracking_uri(\"http://127.0.0.1:5000\")\n",
    "\n",
    "# MLflow „ÅÆÂÆüÈ®ì„ÇíË®≠ÂÆö\n",
    "mlflow.set_experiment(\"250311_xceptionnet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# „É¢„Éá„É´„ÇíÂÆöÁæ©\n",
    "import timm\n",
    "import torch.nn as nn\n",
    "\n",
    "num_classes = len(set(train_labels))\n",
    "\n",
    "def create_model(model_name=\"legacy_xception\", num_classes=num_classes):\n",
    "    model = timm.create_model(model_name, pretrained=True, num_classes=num_classes)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    train_correct = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    for imgs, labels in train_loader:\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(imgs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        batch_size = labels.size(0)\n",
    "        train_loss += loss.item() * batch_size \n",
    "        train_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "        total_samples += batch_size\n",
    "    \n",
    "    train_loss /= total_samples\n",
    "    train_acc = train_correct/ total_samples\n",
    "    \n",
    "    return train_loss, train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_correct = 0\n",
    "    total_val_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in val_loader:\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "            #labels = labels.squeeze().long()  # „É©„Éô„É´„Çí1D„Å´Â§âÊèõ\n",
    "            outputs = model(imgs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            batch_size = labels.size(0)\n",
    "            val_loss += loss.item() * batch_size\n",
    "            val_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "            total_val_samples += batch_size\n",
    "        \n",
    "    val_loss /= total_val_samples\n",
    "    val_acc = val_correct / total_val_samples\n",
    "        \n",
    "    return val_loss, val_acc,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# „É¢„Éá„É´„ÅÆ‰ΩúÊàê„ÉªË®ìÁ∑¥„ÉªË©ï‰æ°„Éª„É≠„Ç∞Ë®òÈå≤\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "def objective(train_loader, val_loader, num_classes, num_epochs):\n",
    "    set_seed(42)\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    model = create_model(num_classes=num_classes).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer =optim.Adam(model.parameters(), lr=0.0001)\n",
    "    scheduler = StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "\n",
    "    best_val_loss = float(\"inf\")\n",
    "    best_epoch = 0\n",
    "    best_acc = 0\n",
    "    best_model_state = None\n",
    "\n",
    "    with mlflow.start_run() as run:\n",
    "        mlflow.log_param(\"learning_rate\", 0.0001)\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            train_loss, train_acc = train(model, train_loader, criterion, optimizer, device)\n",
    "            val_loss, val_acc = evaluate(model, val_loader, criterion, device)\n",
    "\n",
    "            mlflow.log_metric(\"train_loss\", train_loss, step=epoch)\n",
    "            mlflow.log_metric(\"train_acc\", train_acc, step=epoch)\n",
    "            mlflow.log_metric(\"val_loss\", val_loss, step=epoch)\n",
    "            mlflow.log_metric(\"val_acc\", val_acc, step=epoch)\n",
    "\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                best_epoch = epoch\n",
    "                best_acc = val_acc\n",
    "                best_model_state = model.state_dict()\n",
    "                mlflow.pytorch.log_model(model, \"best_model\")\n",
    "                \n",
    "            scheduler.step()\n",
    "\n",
    "            print(f\"Epoch {epoch}: Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "        return best_epoch, best_val_loss, best_acc, best_model_state            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Train Loss: nan, Train Acc: 0.0327, Val Loss: nan, Val Acc: 0.0326\n",
      "Epoch 1: Train Loss: nan, Train Acc: 0.0327, Val Loss: nan, Val Acc: 0.0326\n",
      "Epoch 2: Train Loss: nan, Train Acc: 0.0327, Val Loss: nan, Val Acc: 0.0326\n",
      "Epoch 3: Train Loss: nan, Train Acc: 0.0327, Val Loss: nan, Val Acc: 0.0326\n",
      "Epoch 4: Train Loss: nan, Train Acc: 0.0327, Val Loss: nan, Val Acc: 0.0326\n",
      "Epoch 5: Train Loss: nan, Train Acc: 0.0327, Val Loss: nan, Val Acc: 0.0326\n",
      "Epoch 6: Train Loss: nan, Train Acc: 0.0327, Val Loss: nan, Val Acc: 0.0326\n",
      "Epoch 7: Train Loss: nan, Train Acc: 0.0327, Val Loss: nan, Val Acc: 0.0326\n",
      "üèÉ View run rogue-roo-650 at: http://127.0.0.1:5000/#/experiments/1/runs/5daf8e5fc9544ab3ad91230827bbbcba\n",
      "üß™ View experiment at: http://127.0.0.1:5000/#/experiments/1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[73], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[0;32m----> 2\u001b[0m best_epoch, best_val_loss, best_acc, best_model_state \u001b[38;5;241m=\u001b[39m objective(train_loader, val_loader, num_classes, num_epochs)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest Epoch:\u001b[39m\u001b[38;5;124m\"\u001b[39m, best_epoch)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest Val Loss:\u001b[39m\u001b[38;5;124m\"\u001b[39m, best_val_loss)\n",
      "Cell \u001b[0;32mIn[72], line 24\u001b[0m, in \u001b[0;36mobjective\u001b[0;34m(train_loader, val_loader, num_classes, num_epochs)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[1;32m     23\u001b[0m     train_loss, train_acc \u001b[38;5;241m=\u001b[39m train(model, train_loader, criterion, optimizer, device)\n\u001b[0;32m---> 24\u001b[0m     val_loss, val_acc \u001b[38;5;241m=\u001b[39m evaluate(model, val_loader, criterion, device)\n\u001b[1;32m     26\u001b[0m     mlflow\u001b[38;5;241m.\u001b[39mlog_metric(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m, train_loss, step\u001b[38;5;241m=\u001b[39mepoch)\n\u001b[1;32m     27\u001b[0m     mlflow\u001b[38;5;241m.\u001b[39mlog_metric(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_acc\u001b[39m\u001b[38;5;124m\"\u001b[39m, train_acc, step\u001b[38;5;241m=\u001b[39mepoch)\n",
      "Cell \u001b[0;32mIn[71], line 8\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(model, val_loader, criterion, device)\u001b[0m\n\u001b[1;32m      5\u001b[0m total_val_samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m----> 8\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m imgs, labels \u001b[38;5;129;01min\u001b[39;00m val_loader:\n\u001b[1;32m      9\u001b[0m         imgs, labels \u001b[38;5;241m=\u001b[39m imgs\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat16), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     10\u001b[0m         \u001b[38;5;66;03m#labels = labels.squeeze().long()  # „É©„Éô„É´„Çí1D„Å´Â§âÊèõ\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/env_py311_p5/lib/python3.11/site-packages/torch/utils/data/dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    707\u001b[0m ):\n",
      "File \u001b[0;32m/opt/conda/envs/env_py311_p5/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1448\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1445\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[1;32m   1447\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1448\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_data()\n\u001b[1;32m   1449\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1450\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[1;32m   1451\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/env_py311_p5/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1402\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1400\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m   1401\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_thread\u001b[38;5;241m.\u001b[39mis_alive():\n\u001b[0;32m-> 1402\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_get_data()\n\u001b[1;32m   1403\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m   1404\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m/opt/conda/envs/env_py311_p5/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1243\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1230\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m_utils\u001b[38;5;241m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[1;32m   1231\u001b[0m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[1;32m   1232\u001b[0m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1240\u001b[0m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[1;32m   1241\u001b[0m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[1;32m   1242\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1243\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_queue\u001b[38;5;241m.\u001b[39mget(timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m   1244\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[1;32m   1245\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1246\u001b[0m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[1;32m   1247\u001b[0m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[1;32m   1248\u001b[0m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/env_py311_p5/lib/python3.11/queue.py:180\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m remaining \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n\u001b[1;32m    179\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[0;32m--> 180\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnot_empty\u001b[38;5;241m.\u001b[39mwait(remaining)\n\u001b[1;32m    181\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get()\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnot_full\u001b[38;5;241m.\u001b[39mnotify()\n",
      "File \u001b[0;32m/opt/conda/envs/env_py311_p5/lib/python3.11/threading.py:331\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 331\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;28;01mTrue\u001b[39;00m, timeout)\n\u001b[1;32m    332\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    333\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "best_epoch, best_val_loss, best_acc, best_model_state = objective(train_loader, val_loader, num_classes, num_epochs)\n",
    "\n",
    "print(\"Best Epoch:\", best_epoch)\n",
    "print(\"Best Val Loss:\", best_val_loss)\n",
    "print(\"Best Acc:\", best_acc)\n",
    "print(\"Best Model Sate:\", best_model_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_py311_p5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
